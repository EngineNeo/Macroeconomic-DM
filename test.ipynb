{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stephen Bannon Reassures Conservatives Uneasy About Trump\n"
     ]
    }
   ],
   "source": [
    "from newsplease import NewsPlease\n",
    "article = NewsPlease.from_url('https://www.nytimes.com/2017/02/23/us/politics/cpac-stephen-bannon-reince-priebus.html?hp')\n",
    "print(article.title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import csv\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "\n",
    "# Set env variables\n",
    "APCA_API_KEY_ID = os.getenv(\"APCA_API_KEY_ID\")\n",
    "APCA_API_SECRET_KEY = os.getenv(\"APCA_API_SECRET_KEY\")\n",
    "\n",
    "# Keywords related to macroeconomic indicators\n",
    "keywords = [\n",
    "    \"Gross Domestic Product\", \"GDP\", \"Unemployment Rate\", \"Inflation Rate\",\n",
    "    \"Consumer Price Index\", \"CPI\", \"Producer Price Index\", \"PPI\", \"Interest Rates\",\n",
    "    \"Balance of Trade\", \"Government Debt\", \"Budget Deficit\", \"Surplus\", \"Exchange Rates\",\n",
    "    \"Money Supply\", \"Industrial Production\", \"Retail Sales\", \"Housing Starts\"\n",
    "]\n",
    "\n",
    "# Prepare headers for the HTTP request\n",
    "headers = {\n",
    "    'APCA-API-KEY-ID': APCA_API_KEY_ID,\n",
    "    'APCA-API-SECRET-KEY': APCA_API_SECRET_KEY,\n",
    "}\n",
    "\n",
    "# Fetch and filter news and write to csv\n",
    "def fetch_and_filter_news(start_date, end_date, batch_size=30):\n",
    "    next_page_token = None\n",
    "    has_more = True\n",
    "    with open('macroeconomic_news.csv', 'w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"Date\", \"Headline\", \"Content\"])\n",
    "\n",
    "        while start_date < end_date:\n",
    "            batch_end_date = start_date + timedelta(days=30)\n",
    "            while has_more:\n",
    "                params = {\n",
    "                    'start': start_date.strftime('%Y-%m-%d'),\n",
    "                    'end': batch_end_date.strftime('%Y-%m-%d'),\n",
    "                    'include_content': 'true',\n",
    "                    'limit': batch_size,\n",
    "                }\n",
    "\n",
    "                if next_page_token:\n",
    "                    params['page_token'] = next_page_token\n",
    "\n",
    "                response = requests.get('https://data.alpaca.markets/v1beta1/news', headers=headers, params=params)\n",
    "\n",
    "                if response.status_code == 200:\n",
    "                    news_batch = response.json()\n",
    "                    for article in news_batch['news']:\n",
    "                        # Keyword check\n",
    "                        if any(keyword in article['headline'] or keyword in article['summary'] for keyword in keywords):\n",
    "                            date = datetime.fromisoformat(article['created_at']).strftime('%m/%d/%Y')\n",
    "                            headline = article['headline']\n",
    "                            content = article['summary']\n",
    "                            writer.writerow([date, headline, content])\n",
    "                            print(date, headline, content)\n",
    "\n",
    "                    next_page_token = news_batch.get('next_page_token')\n",
    "                    has_more = next_page_token is not None\n",
    "                    if not has_more:  # Reset for the next batch\n",
    "                        start_date += timedelta(days=30)  # Next 30 days\n",
    "                        next_page_token = None  # Reset pagination token\n",
    "                        has_more = True  # Reset has_more\n",
    "                else:\n",
    "                    print(\"Failed to fetch news articles\", response.status_code)\n",
    "                    break\n",
    "\n",
    "start_date = datetime.now() - timedelta(days=365*3)\n",
    "end_date = datetime.now()\n",
    "\n",
    "fetch_and_filter_news(start_date, end_date, batch_size=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import csv\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Set env variables\n",
    "APCA_API_KEY_ID = os.getenv(\"APCA_API_KEY_ID\")\n",
    "APCA_API_SECRET_KEY = os.getenv(\"APCA_API_SECRET_KEY\")\n",
    "\n",
    "# Keywords related to macroeconomic indicators\n",
    "keywords = [\n",
    "    \"Gross Domestic Product\", \"GDP\", \"Unemployment Rate\", \"Inflation Rate\",\n",
    "    \"Consumer Price Index\", \"CPI\", \"Producer Price Index\", \"PPI\", \"Interest Rates\",\n",
    "    \"Balance of Trade\", \"Government Debt\", \"Budget Deficit\", \"Surplus\", \"Exchange Rates\",\n",
    "    \"Money Supply\", \"Industrial Production\", \"Retail Sales\", \"Housing Starts\"\n",
    "]\n",
    "\n",
    "# Prepare headers for the HTTP request\n",
    "headers = {\n",
    "    'APCA-API-KEY-ID': APCA_API_KEY_ID,\n",
    "    'APCA-API-SECRET-KEY': APCA_API_SECRET_KEY,\n",
    "}\n",
    "\n",
    "# Fetch and filter news and write to csv\n",
    "def fetch_and_filter_news(end_date, batch_size=30):\n",
    "    next_page_token = None\n",
    "    has_more = True\n",
    "    request_count = 0\n",
    "    max_requests_per_minute = 200\n",
    "    minute_start_time = time.time()\n",
    "    retry_attempts = 3  # Maximum number of retry attempts for a failed request\n",
    "\n",
    "    with open('macroeconomic_news.csv', 'w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"Date\", \"Headline\", \"Content\"])\n",
    "\n",
    "        while end_date > start_date:\n",
    "            batch_start_date = end_date - timedelta(days=30)\n",
    "            while has_more:\n",
    "                # Check rate limit\n",
    "                if request_count >= max_requests_per_minute:\n",
    "                    elapsed_time = time.time() - minute_start_time\n",
    "                    if elapsed_time < 60:\n",
    "                        time.sleep(60 - elapsed_time)\n",
    "                    request_count = 0\n",
    "                    minute_start_time = time.time()\n",
    "\n",
    "                params = {\n",
    "                    'start': batch_start_date.strftime('%Y-%m-%d'),\n",
    "                    'end': end_date.strftime('%Y-%m-%d'),\n",
    "                    'include_content': 'true',\n",
    "                    'limit': batch_size,\n",
    "                }\n",
    "\n",
    "                if next_page_token:\n",
    "                    params['page_token'] = next_page_token\n",
    "\n",
    "                for attempt in range(retry_attempts):\n",
    "                    response = requests.get('https://data.alpaca.markets/v1beta1/news', headers=headers, params=params)\n",
    "                    request_count += 1\n",
    "\n",
    "                    if response.status_code == 200:\n",
    "                        news_batch = response.json()\n",
    "                        for article in news_batch['news']:\n",
    "                            # Keyword check\n",
    "                            if any(keyword in article['headline'] or keyword in article['summary'] for keyword in keywords):\n",
    "                                date = datetime.fromisoformat(article['created_at']).strftime('%m/%d/%Y')\n",
    "                                headline = article['headline']\n",
    "                                content = article['summary']\n",
    "                                writer.writerow([date, headline, content])\n",
    "                                print(date, headline, content)\n",
    "\n",
    "                        next_page_token = news_batch.get('next_page_token')\n",
    "                        has_more = next_page_token is not None\n",
    "                        if not has_more:  # Reset for the next batch\n",
    "                            end_date -= timedelta(days=30)  # Previous 30 days\n",
    "                            next_page_token = None  # Reset pagination token\n",
    "                            has_more = True  # Reset has_more\n",
    "                        break  # Break out of retry loop on success\n",
    "                    else:\n",
    "                        print(f\"Failed to fetch news articles {response.status_code} on attempt {attempt + 1}\")\n",
    "                        if response.status_code == 400:\n",
    "                            print(\"Bad request, skipping this batch.\")\n",
    "                            break\n",
    "                        time.sleep(2 ** attempt)  # Exponential backoff\n",
    "                else:\n",
    "                    # No more retries\n",
    "                    end_date -= timedelta(days=30)\n",
    "                    has_more = False\n",
    "                    next_page_token = None\n",
    "\n",
    "start_date = datetime.now() - timedelta(days=365*5)\n",
    "end_date = datetime.now()\n",
    "\n",
    "fetch_and_filter_news(end_date, batch_size=30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "webscraping",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
